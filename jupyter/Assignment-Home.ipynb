{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `OscaRobber`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to take the data hosted on IMDB (~12GB) and perform a comprehensive analysis on it in order to get insight into the film industry and be able to predict THE successful movie later on. The dataset is semistructured so it requires a lot of cleaning and preprocessing tasks in order to make it accessible for the entire project, hence we will use some of the key techniques introduced in the course. First of all, we will perform analysis on the co-stardom network of science fiction movies, that is, we will look how movies and actors interact, and how fancy the structure of the film industry really is. We will make all kind of graphs that will make it clear on what is important - what kind of movies do the actors prefer, what kind of actors the movies prefer, how many \"friends\" do the actor have, how international is the film industry, etc. Afterwards, we will dig a little bit deeper into contents of movies to answer whether there is a correlation between native keywords provided by IMDB and plots, but also, whether actors tend to act in movies of the same contents. This way we will connect text analysis with network analysis to discover the picture more in detail. We will also take into consideration sentiments of user reviews and analyze its distribution, but also their lexical diversity. ***Important: You will find the documentation on each analysis step in the corresponding notebook below.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Part I - Data preparation](http://nbviewer.jupyter.org/github/polakowo/oscarobber/blob/master/jupyter/Assignment-PartI-DataPreparation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we decide on libraries and data structures we will use and perform the initial preprocessing of the data to a shape which is best for use in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Part II - Network analysis](http://nbviewer.jupyter.org/github/polakowo/oscarobber/blob/master/jupyter/Assignment-PartII-NetworkAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will perform an analysis on the network of science fiction movies. We will populate the network with data from MongoDB, derive network properties, test assortativity and uncover hidden communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Part III - Text analysis](http://nbviewer.jupyter.org/github/polakowo/oscarobber/blob/master/jupyter/Assignment-PartIII-TextAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to analyze contents of movies. We will check whether movies in which an actor has played have more similar content, based on keywords provided by IMDB and keywords generated from plots, than based on other attributes. We will also discover sentiments of user reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of data from IMDB was the most difficult part, as the data provided by IMDB platform was semi-structured and needed an infrastructure to extract the most useful information (using regular expressions, mapping, reducing, etc). A lot of read-operations were performed. Different data types and relationships between tables made every resulting dataset diverse, but after several attempts we've finally brought structure into it and made the whole pipeline uniform. Each SQL statement is time-expensive (up to 120 sec) so we decided to change to a NoSQL database which accepts indexing (MongoDB). The second most problematic part was the network analysis, where operations like the extraction of bipartite projections and solving various NP-problems couldn't be parallelized, hence we worked on one genre (*science fiction*) only. There were almost no troubles extracting features form texts though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we've done right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we are really proud of is our super-flexible infrastructure: by changing just a couple of variables you are able to perform analysis on an entire different genre, film, or actor. Every piece of code was aimed at flexibility and customization. You can put almost any kind of information into a MongoDB using our `MongoHelper` class, and discover the magic behind it using `NetworkHelper` class. No constants or hard-coded variables were used in any of the helper classes - you can customize every function call by passing different keyword parameters. The code could have been a fundament for an entire new Python library (we actually think about this). But also the amount of work we've invested in producing beautiful plots pays off (alone the function for drawing every-kind-of graphs takes 150 lines of code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we haven't done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of deadlines and a monstrous perfectionism, we were not be able to implement some fancy features we would so like to: an interactive website where user can type in a genre, movie or actor and get the full analysis in return. We also didn't train a set of classifiers to predict things - we looked at Kaggle competitions and did notice an almost zero classifier performance for the most of attributes, hence we ignored this step. We decided to dig into the analysis deeper though. It is all about quality, not quantity, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
